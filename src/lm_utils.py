import logging
import random
import string
from functools import lru_cache
from typing import Literal

import torch
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm
from transformers import DynamicCache, PreTrainedTokenizerBase

from src.io_utils import free_vram


def generate_batched_completions(*args, **kwargs):
    raise NotImplementedError("Please migrate to generate_ragged (direct replacement), or generate_ragged_batched (with dynamic batch size adjustment).")


@torch.no_grad()
def generate_ragged_batched(
    model,
    tokenizer,
    token_list=None,
    embedding_list=None,
    initial_batch_size=64,
    use_cache=True,
    verbose=False,
    **kwargs,
) -> list[str]:
    """
    Generate completions for input_list, which can be either embeddings or tokens.
    Dynamically adjust batch size if an OOM error occurs.

    Args:
        model: The language model to use for generation.
        tokenizer: The tokenizer for the model.
        token_list: List of embeddings or tokens to generate from.
        embedding_list: List of embeddings or tokens to generate from.
        initial_batch_size: Starting batch size for generation.
        **kwargs: Additional arguments will be passed to `generate_ragged`.
    Returns:
        List of outputs generated by the model.
    """
    if token_list is not None and embedding_list is not None:
        raise ValueError("Only one of token_list or embedding_list should be provided.")
    input_type = "tokens" if token_list is not None else "embeddings"
    input_list = token_list if token_list is not None else embedding_list

    def func(chunk):
        """Wrapper function to handle a single chunk of inputs."""
        return generate_ragged(
            model=model,
            tokenizer=tokenizer,
            token_list=chunk if input_type == "tokens" else None,
            embedding_list=chunk if input_type == "embeddings" else None,
            use_cache=use_cache,
            **kwargs,
        )
    outputs = with_max_batchsize(func, input_list, initial_batch_size, verbose=verbose)
    return outputs


def with_max_batchsize(function, input, initial_batch_size=128, verbose=False):
    """
    Dynamically adjust batch size if an OOM error occurs.
    TODO: Try increasing batch size again if we have enough VRAM.

    Args:
        function:
            A single-argument function to run.
            Takes a tensor or list of tensors and returns a tensor or list of tensors.
        input:
            The input to pass to the function, first dimension is batch dimension.
        initial_batch_size:
            Starting batch size for execution.
        verbose:
            Whether to print progress.
    Returns:
        The output of the function.
    """
    outputs = []
    i = 0
    batch_size = min(initial_batch_size, len(input))
    pbar = tqdm(total=len(input), desc=f"Running function b={batch_size}") if verbose else None
    while i < len(input):
        chunk = input[i : i + batch_size]
        try:
            free_vram()
            output = function(chunk)
            outputs.append(output)
            i += batch_size  # Move to the next batch
            if verbose:
                pbar.update(batch_size)
        except torch.cuda.OutOfMemoryError:
            # If we hit OOM, reduce batch size and retry the same chunk
            batch_size = batch_size // 2
            if verbose:
                pbar.set_description(f"Running function b={batch_size}")
            if batch_size < 1:
                raise RuntimeError(
                    "OOM even with batch_size=1; cannot generate further."
                )
    if verbose:
        pbar.close()
    if all(isinstance(x, torch.Tensor) for x in outputs):
        outputs = torch.cat(outputs, dim=0)
        assert len(outputs) == len(input)
    elif all(isinstance(x, tuple) for x in outputs):
        # Transpose and concatenate tuple outputs
        outputs = tuple(torch.cat([x[i] for x in outputs], dim=0) for i in range(len(outputs[0])))
        assert all(len(o) == len(input) for o in outputs)
    else:
        outputs = [item for sublist in outputs for item in sublist]
        assert len(outputs) == len(input)
    return outputs


@torch.no_grad
def generate_ragged(
    model,
    tokenizer,
    embedding_list=None,
    token_list=None,
    max_new_tokens: int = 256,
    return_tokens=False,
    padding_side="right",
    use_cache=True,
    temperature=0.0,
    top_p=1.0,
    top_k=0,
) -> list[str] | torch.Tensor:
    """
    Generate completions for multiple prompts in a single batch.
    No KV-cache for left-padding yet.
    Heavily tested across models to be close to individual generations.
    This is far from trivial due to various padding (left/right) and masking issues.
    The final function is still not identical to individual generations, but it is close.
    The reason for this is probably that attention masks typically don't use -inf for
    masked tokens, but instead have values like -65504 for float16.
    This can lead to small differences in the final logits and thus the generated tokens.
    We are much closer to individual generations than HF model.generate, which often
    fails in mysterious ways for LLama & Qwen models. Generations for CircuitBreakers
    often look weird, but are actually just what the model would output with `generate`
    as well.

    Number of generations that are the same as single-batch:
        Model Name                         This function    HF generate
        cais/zephyr_7b_r2d2                      100/100        100/100
        ContinuousAT/Llama-2-7B-CAT              100/100         39/100
        ContinuousAT/Phi-CAT                      90/100         95/100
        ContinuousAT/Zephyr-CAT                  100/100        100/100
        google/gemma-2-2b-it                      55/100         56/100
        meta-llama/Meta-Llama-3.1-8B-Instruct     62/100         11/100
        meta-llama/Llama-2-7b-chat-hf             88/100         30/100
        microsoft/Phi-3-mini-4k-instruct          53/100         50/100
        mistralai/Mistral-7B-Instruct-v0.3        83/100         79/100
        qwen/Qwen2-7B-Instruct                    78/100         19/100
        ---------------------------------------------------------------
        Total                                   809/1000       579/1000

    Args:
        model: A pretrained model.
        tokenizer: A pretrained tokenizer.
        embedding_list: list[torch.Tensor], optional
            A list of embeddings for each prompt. Should not be padded and can be of different lengths.
        token_list: list[torch.Tensor], optional
            A list of tokens for each prompt. Should not be padded and can be of different lengths.
        max_new_tokens: The maximum number of tokens to generate for each prompt.
    Returns:
        A list of completions for each prompt.
    """
    if (embedding_list is None) == (token_list is None):
        raise ValueError("One of embedding_list or token_list must be provided.")
    if embedding_list is not None:
        assert all(e.ndim == 2 for e in embedding_list), "Embeddings must be 2D."
        embedding_list = [e.to(model.device) for e in embedding_list]
    if token_list is not None:
        assert all(t.ndim == 1 for t in token_list), "Tokens must be 1D."
        token_list = [t.to(model.device) for t in token_list]
        embedding_list = [
            model.get_input_embeddings()(t.unsqueeze(0))[0] for t in token_list
        ]
    assert embedding_list is not None
    # TODO: Implement KV-caching for Gemma
    if use_cache and "gemma-2" in model.name_or_path:
        logging.warning("KV-cache not implemented for Gemma 2. Disabling cache.")
        use_cache = False

    def sample_next_token(logits: torch.Tensor) -> torch.Tensor:
        if temperature > 0.0:
            logits = logits / temperature
            if top_p < 1.0:
                logits = top_p_filtering(logits, top_p)
            if top_k > 0:
                logits = top_k_filtering(logits, top_k)
            next_tokens = torch.multinomial(logits.softmax(dim=-1), num_samples=1)
        else:
            next_tokens = logits.argmax(dim=-1)
        return next_tokens

    B = len(embedding_list)
    tokens = []
    generation_completed = torch.zeros(B, dtype=torch.bool)
    if padding_side == "left":
        if use_cache:
            raise NotImplementedError("KV-cache not implemented for left padding.")
        # Add left padding
        embeddings = pad_sequence(
            [e.flip(0) for e in embedding_list], batch_first=True, padding_value=0
        ).flip(1)
        padded_embeddings = F.pad(embeddings, (0, 0, 0, max_new_tokens))
        # Create attention mask and position ids
        lengths = [
            {
                "padding": embeddings.size(1) - e.size(0),
                "generation": max_new_tokens - e.size(0),
            }
            for e in embedding_list
        ]
        attention_mask = torch.stack(
            [
                torch.cat([torch.zeros(pl["padding"]), torch.ones([pl["generation"]])])
                for pl in lengths
            ]
        ).to(model.device)
        position_ids = torch.stack(
            [
                torch.cat([torch.zeros(pl["padding"]), torch.arange(pl["generation"])])
                for pl in lengths
            ]
        ).long().to(model.device)
        next_token_idx = embeddings.size(1)
        for i in range(max_new_tokens):
            outputs = model(
                inputs_embeds=padded_embeddings[:, :next_token_idx],
                attention_mask=attention_mask[:, :next_token_idx],
                position_ids=position_ids[:, :next_token_idx],
            )
            logits = outputs.logits[torch.arange(B), -1]
            next_tokens = sample_next_token(logits)
            padded_embeddings[torch.arange(B), next_token_idx] = (
                model.get_input_embeddings()(next_tokens).detach()
            )
            tokens.append(next_tokens.cpu())
            generation_completed |= next_tokens.cpu() == tokenizer.eos_token_id
            if generation_completed.all():
                logging.info(f"Early exit after {i}/{max_new_tokens} tokens.")
                break
            next_token_idx += 1
    elif padding_side == "right":
        # Add right padding
        embeddings = pad_sequence(
            [e for e in embedding_list], batch_first=True, padding_value=0
        )
        padded_embeddings = F.pad(embeddings, (0, 0, 0, max_new_tokens))
        next_token_idx = torch.tensor([e.size(0) for e in embedding_list])

        if use_cache:
            # This is the hot path so we have additional optimizations here.
            # As we generate tokens, we keep track of which prompts are completed,
            # and only generate tokens for the active prompts.
            # This is slightly (~20%) slower if all prompts have similar length,
            # but faster if prompts have different lengths and **saves VRAM**.

            # Fill prefix cache
            past_key_values = DynamicCache()
            if next_token_idx.min() > 1:
                model(
                    inputs_embeds=padded_embeddings[:, : next_token_idx.min() - 1],
                    past_key_values=past_key_values,
                    use_cache=True,
                )
            for i in range(max_new_tokens):
                # Caching with right padding is a bit tricky:
                # We have to feed more than one token at each forward pass :(.
                # Instead, we feed a 'window' from the last token of the shortest prompt
                # to the last token of the longest prompt.
                # This means that caching works best when sequences have similar length.
                active_mask = ~generation_completed
                active_mask_idx = torch.arange(B)[active_mask]
                active_embeddings = padded_embeddings[active_mask, next_token_idx.min() - 1 : next_token_idx.max()]
                logits = model(
                    inputs_embeds=active_embeddings,
                    past_key_values=past_key_values,
                    use_cache=True,
                ).logits

                next_tokens = torch.full((B,), tokenizer.eos_token_id, device=model.device)
                next_token_idx_active = next_token_idx[active_mask]
                logits = logits[
                    torch.arange(logits.size(0)),
                    next_token_idx_active - next_token_idx.min()
                ]
                next_tokens[active_mask] = sample_next_token(logits)
                padded_embeddings[active_mask_idx, next_token_idx_active] = model.get_input_embeddings()(next_tokens[active_mask])
                tokens.append(next_tokens.cpu())
                continue_mask = (next_tokens.cpu() != tokenizer.eos_token_id)[active_mask]
                # have to manually crop the past_key_values to the correct length
                # since we only add a single step at a time
                for j in range(len(past_key_values.key_cache)):
                    past_key_values.key_cache[j] = past_key_values.key_cache[j][continue_mask, :, :next_token_idx.min()]
                    past_key_values.value_cache[j] = past_key_values.value_cache[j][continue_mask, :, :next_token_idx.min()]

                generation_completed |= next_tokens.cpu() == tokenizer.eos_token_id
                if generation_completed.all():
                    logging.info(f"Early exit after {i}/{max_new_tokens} tokens.")
                    break
                next_token_idx += 1
        else:
            for i in range(max_new_tokens):
                outputs = model(inputs_embeds=padded_embeddings[:, : next_token_idx.max()])
                logits = outputs.logits[torch.arange(B), next_token_idx - 1]
                next_tokens = sample_next_token(logits)
                padded_embeddings[torch.arange(B), next_token_idx] = (
                    model.get_input_embeddings()(next_tokens).detach()
                )
                tokens.append(next_tokens.cpu())
                generation_completed |= next_tokens.cpu() == tokenizer.eos_token_id
                if generation_completed.all():
                    logging.info(f"Early exit after {i}/{max_new_tokens} tokens.")
                    break
                next_token_idx += 1
    else:
        raise ValueError(f"Unknown padding_side: {padding_side}")

    tokens = torch.stack(tokens, dim=0).T
    if return_tokens:
        return tokens
    completion = tokenizer.batch_decode(tokens, skip_special_tokens=False)
    completion = [c.split(tokenizer.eos_token)[0] for c in completion]
    return completion


@torch.no_grad
def get_batched_losses(
    model,
    targets,
    embedding_list=None,
    token_list=None,
    padding_side="right",
) -> list[torch.Tensor]:
    """
    Get per-timestep losses for multiple ragged prompts in a single batch.
    No KV-cache for now.

    Args:
        model: A pretrained model.
        targets: A list of 1D tensors containing the target tokens for each prompt.
        embedding_list: list[torch.Tensor], optional
            A list of embeddings for each prompt. Should not be padded and can be of different lengths.
        token_list: list[torch.Tensor], optional
            A list of tokens for each prompt. Should not be padded and can be of different lengths.
        max_new_tokens: The maximum number of tokens to generate for each prompt.
    Returns:
        A list of completions for each prompt.
    """
    if (embedding_list is None) == (token_list is None):
        raise ValueError("Either embedding_list or token_list must be provided.")
    if embedding_list is not None:
        assert all(e.ndim == 2 for e in embedding_list), "Embeddings must be 2D."
        embedding_list = [e.to(model.device) for e in embedding_list]
    if token_list is not None:
        assert all(t.ndim == 1 for t in token_list), "Tokens must be 1D."
        token_list = [t.to(model.device) for t in token_list]
        embedding_list = [
            model.get_input_embeddings()(t.unsqueeze(0))[0] for t in token_list
        ]
    assert embedding_list is not None
    assert all(t.ndim == 1 for t in targets), "Targets must be 1D."
    targets = [t.to(model.device) for t in targets]

    # We first pad the embeddings to the maximum context length of the model.
    B = len(embedding_list)
    if padding_side == "left":
        print("Warning: Padding side 'left' is not recommended for get_batched_losses as it may yield nans.")
        # Add left padding
        embeddings = pad_sequence(
            [e.flip(0) for e in embedding_list], batch_first=True, padding_value=0
        ).flip(1)
        targets_padded = pad_sequence(
            [t.flip(0) for t in targets], batch_first=True, padding_value=0
        ).flip(1)
        # Create attention mask and position ids
        lengths = [
            {
                "padding": embeddings.size(1) - e.size(0),
                "generation": e.size(0),
            }
            for e in embedding_list
        ]
        attention_mask = torch.stack(
            [
                torch.cat([torch.zeros(pl["padding"]), torch.ones([pl["generation"]])])
                for pl in lengths
            ]
        ).to(model.device)
        position_ids = torch.stack(
            [
                torch.cat(
                    [torch.zeros(pl["padding"]), torch.arange(pl["generation"])]
                )
                for pl in lengths
            ]
        ).long().to(model.device)

        outputs = model(
            inputs_embeds=embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
        ).logits
        losses = F.cross_entropy(
            outputs.reshape(-1, outputs.size(-1)),
            targets_padded.view(-1),
            reduction="none",
        )
        losses = losses.view(B, -1)
        losses = [losses[i, -t.size(0) : -1] for i, t in enumerate(targets)]
    elif padding_side == "right":
        # Add right padding
        embeddings = pad_sequence(
            [e for e in embedding_list], batch_first=True, padding_value=0
        )
        targets_padded = pad_sequence(
            [t for t in targets], batch_first=True, padding_value=0
        )
        outputs = model(inputs_embeds=embeddings).logits
        losses = F.cross_entropy(
            outputs.reshape(-1, outputs.size(-1)),
            targets_padded.view(-1),
            reduction="none",
        )
        losses = losses.view(B, -1)
        losses = [losses[i, : t.size(0) - 1] for i, t in enumerate(targets)]
    else:
        raise ValueError(f"Unknown padding_side: {padding_side}")

    return losses


def prepare_tokens(
    tokenizer: PreTrainedTokenizerBase,
    prompt: str,
    target: str,
    attack: str | None = None,
    placement: Literal["prompt", "suffix"] = "suffix",
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """For many attacks, we need to figure out how exactly to tokenize the input.
    Since only some models add a space or various control tokens, we have to figure
    out the exact format. We want to make sure that the first generated token is
    exactly 'Sure', and not a space or control token.

    We thus chunk the sequence into the following 5 parts (some of which may be empty):

    [PRE] + [Prompt] + [Attack] + [POST] + [Target]

    Treating prompt and attack separately is important for optimization, as we only
    want to optimize the attack part.
    Tested with:
        cais/zephyr_7b_r2d2
        google/gemma-2-2b-it
        HuggingFaceH4/zephyr-7b-beta
        meta-llama/Llama-2-7b-chat-hf
        meta-llama/Meta-Llama-3-8B-Instruct
        meta-llama/Meta-Llama-3.1-8B-Instruct
        microsoft/Phi-3-mini-4k-instruct
        mistralai/Mistral-7B-Instruct-v0.3
        qwen/Qwen2-7B-Instruct


    Parameters:
    - tokenizer: The tokenizer to use.
    - prompt: The prompt string to use.
    - target: The target string to use.
    - attack: The attack string to use.
    - placement: Where to place the attack. Can be either "prompt" or "suffix".

    Returns:
    - pre_tokens: The tokens before the prompt.
    - prompt_tokens: The tokens of the prompt.
    - attack_tokens: The tokens of the attack. <- optimize these
    - post_tokens: The tokens after the attack.
    - target_tokens: The tokens of the target string. <- apply loss here
    """
    if placement == "prompt":
        attack, prompt = prompt, ""
    elif attack is None:
        raise ValueError("If placement is 'suffix', attack must be provided.")

    # Some tokenizers and templates (e.g., allenai/Llama-3.1-Tulu-3-8B-DPO) need more
    # messages because their tokenization is more likely to have weird splits.
    for num_messages in [100, 1000, 10000]:
        pre_tokens, post_tokens, suffix_tokens = get_pre_post_suffix_tokens(tokenizer, num_messages)
        # Now we look at the actual chat by the user
        chat = [
            {"role": "user", "content": prompt + attack},
            {"role": "assistant", "content": target},
        ]
        tokenized_together = tokenize_chats([chat], tokenizer)[0]
        # We now cut the tokenized sequence into parts step-by-step.
        # First, we remove the prefix and suffix tokens, as we already know the prefix and
        # don't neeed the suffix.
        prompt_attack_post_target = tokenized_together[len(pre_tokens) : -len(suffix_tokens)]
        # We now look for the post tokens. These are between [prompt + attack] and [target].

        # Now, we cut out sliding views from the remaining tokens and check if they match the post tokens.
        sliding_windows = torch.stack([
            prompt_attack_post_target[i:i+len(post_tokens)]
            for i in range(len(prompt_attack_post_target) - len(post_tokens) + 1)
        ])

        # Compare each window with post_tokens
        matches = torch.all(sliding_windows == post_tokens, dim=1)
        # Find the first match index
        match_indices = torch.where(matches)[0]

        if len(match_indices) > 0:
            # Get the first match position
            i = match_indices[0].item()
            prompt_attack_tokens = prompt_attack_post_target[:i]
            target_tokens = prompt_attack_post_target[i + len(post_tokens):]
            break
    else:
        raise ValueError(
            f"Unable to find consistent tokenizer patterns for {tokenizer.name_or_path}"
        )

    chat_no_attack = [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": target},
    ]
    tokenized_together_no_attack = tokenize_chats([chat_no_attack], tokenizer)[0]

    attack_length = len(tokenized_together) - len(tokenized_together_no_attack)

    # OPTIMIZATION: Use direct indexing instead of tensor_split if possible
    prompt_tokens, attack_tokens = torch.tensor_split(
        prompt_attack_tokens, [prompt_attack_tokens.size(0)-attack_length]
    )
    if "llama-2" in tokenizer.name_or_path.lower():
        # LLama 2 models have incorrect templating and need to be fixed manually
        post_tokens = torch.cat([post_tokens, torch.tensor([29871])])

    return pre_tokens, prompt_tokens, attack_tokens, post_tokens, target_tokens


def _make_random_chats(n: int, k: int = 5) -> list[list[dict[str, str]]]:
    """Generate n random chat conversations with k-length messages.

    Returns:
        List of chat conversations, where each conversation is a list of
        user/assistant message dictionaries with random content.
    """
    chars = string.ascii_letters + string.digits + " "

    def generate_random_string() -> str:
        return "".join(random.choices(chars, k=k))

    chats = []
    for _ in range(n):
        chat = [
            {"role": "user", "content": generate_random_string()},
            {"role": "assistant", "content": generate_random_string()},
        ]
        chats.append(chat)

    return chats


def _extract_prefix_middle_suffix(vectors):
    def longest_common_prefix(sequences):
        if not sequences:
            return []
        prefix = sequences[0]
        for seq in sequences[1:]:
            min_len = min(len(prefix), len(seq))
            i = 0
            while i < min_len and prefix[i] == seq[i]:
                i += 1
            prefix = prefix[:i]
            if not prefix:
                return []
        return prefix

    def longest_common_suffix(sequences):
        if not sequences:
            return []
        suffix = sequences[0]
        for seq in sequences[1:]:
            min_len = min(len(suffix), len(seq))
            i = 1
            while i <= min_len and suffix[-i] == seq[-i]:
                i += 1
            if i > 1:
                suffix = suffix[-(i - 1) :]
            else:
                return []
        return suffix

    def longest_common_subsequence(sequences):
        if not sequences:
            return []
        reference = sequences[0]
        n = len(reference)
        # Start with the longest possible substrings and decrease length
        for length in range(n, 0, -1):
            for start in range(n - length + 1):
                candidate = reference[start : start + length]
                if all(
                    any(
                        candidate == seq[i : i + length]
                        for i in range(len(seq) - length + 1)
                    )
                    for seq in sequences[1:]
                ):
                    return candidate
        return []

    sequences = [vec.tolist() for vec in vectors]
    prefix = longest_common_prefix(sequences)
    suffix = longest_common_suffix(sequences)
    # Trim the prefix and suffix from sequences
    sequences_trimmed = [
        seq[len(prefix) : len(seq) - len(suffix) if len(suffix) > 0 else None]
        for seq in sequences
    ]
    middle = longest_common_subsequence(sequences_trimmed)
    return torch.tensor(prefix), torch.tensor(middle), torch.tensor(suffix)


def tokenize_chats(chats: list[list[dict[str, str]]], tokenizer) -> list[torch.Tensor]:
    templates = tokenizer.apply_chat_template(
        chats, tokenize=False, add_generation_prompt=False
    )
    # Sometimes, the chat template adds the BOS token to the beginning of the template.
    # The tokenizer adds it again later, so we need to remove it to avoid duplication.
    if tokenizer.bos_token:
        for i, template in enumerate(templates):
            templates[i] = template.removeprefix(tokenizer.bos_token)

    return [
        tokenizer(t, return_tensors="pt", add_special_tokens=True).input_ids[0]
        for t in templates
    ]


# Generate random messages to find tokenizer patterns, this is ugly but fast
@lru_cache()
def get_pre_post_suffix_tokens(tokenizer, num_messages):
    test_chats = _make_random_chats(num_messages)
    test_tokenized = tokenize_chats(test_chats, tokenizer)
    return _extract_prefix_middle_suffix(test_tokenized)


def top_p_filtering(logits: torch.Tensor, top_p: float) -> torch.Tensor:
    """Filter logits using nucleus (top-p) sampling.

    Parameters
    ----------
    logits: torch.Tensor, shape (B, T, V) or (B, V)
        The logits to filter.
    top_p: float
        The top-p threshold.

    Returns
    -------
    torch.Tensor
    """
    single_token_only = logits.ndim == 2
    if single_token_only:
        logits = logits.unsqueeze(1)
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

    # Remove tokens with cumulative probability above the threshold
    sorted_indices_to_remove = cumulative_probs > top_p
    # Shift indices to the right to keep also the first token above the threshold
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0

    # Scatter sorted tensors to original indexing
    indices_to_remove = sorted_indices_to_remove.scatter(
        -1, sorted_indices, sorted_indices_to_remove
    )
    logits[indices_to_remove] = float('-inf')
    if single_token_only:
        logits = logits.squeeze(1)
    return logits


def top_k_filtering(logits: torch.Tensor, top_k: int) -> torch.Tensor:
    """Filter logits using top-k sampling.

    Parameters
    ----------
    logits: torch.Tensor, shape (B, T, V) or (B, V)
        The logits to filter.
    top_k: int
        The top-k threshold.

    Returns
    -------
    torch.Tensor
        Filtered logits with values below top-k threshold set to -inf.
    """
    single_token_only = logits.ndim == 2
    if single_token_only:
        logits = logits.unsqueeze(1)

    values, _ = torch.topk(logits, top_k)
    # Get minimum value of top-k tokens
    min_values = values[..., -1, None]
    # Zero out everything below min values
    logits[logits < min_values] = float('-inf')

    if single_token_only:
        logits = logits.squeeze(1)
    return logits


def get_disallowed_ids(tokenizer: PreTrainedTokenizerBase, allow_non_ascii: bool, allow_special: bool) -> torch.Tensor:
    disallowed_ids = set()

    def is_ascii(s):
        return s.isascii() and s.isprintable()

    # Important to loop over len(tokenizer), not just tokenizer.vocab_size, because
    # special tokens added post-hoc are not counted to vocab_size.
    if not allow_non_ascii:
        for i in range(len(tokenizer)):
            if not is_ascii(tokenizer.decode([i])):
                disallowed_ids.add(i)

    if not allow_special:
        for i in range(len(tokenizer)):
            if not tokenizer.decode([i], skip_special_tokens=True):
                disallowed_ids.add(i)

    if tokenizer.bos_token_id is not None:
        disallowed_ids.add(tokenizer.bos_token_id)
    if tokenizer.eos_token_id is not None:
        disallowed_ids.add(tokenizer.eos_token_id)
    if tokenizer.pad_token_id is not None:
        disallowed_ids.add(tokenizer.pad_token_id)
    if tokenizer.unk_token_id is not None:
        disallowed_ids.add(tokenizer.unk_token_id)
    disallowed_ids = sorted(list(disallowed_ids))
    return torch.tensor(disallowed_ids)


def filter_suffix(ids: torch.Tensor, tokenizer: PreTrainedTokenizerBase, prompt: str, target: str) -> torch.Tensor:
    """Filters out sequences of token ids that change after retokenization.

    Parameters
    ----------
    ids : Tensor, shape = (search_width, n_optim_ids)
        The token ids to filter
    tokenizer : ~transformers.PreTrainedTokenizer
        The tokenizer to use
    prompt : str
        The prompt to use
    target : str
        The target to use

    Returns
    -------
    filtered_ids : Tensor, shape = (new_search_width, n_optim_ids)
        all token ids that are the same after retokenization
    """
    attacks_decoded = tokenizer.batch_decode(ids)
    filtered_idx = []

    for i, attack in enumerate(attacks_decoded):
        pre_ids, prompt_ids, attack_ids, post_ids, target_ids = prepare_tokens(
            tokenizer,
            prompt,
            target,
            attack=attack,
            placement="suffix",
        )
        if torch.equal(ids[i], attack_ids.to(ids.device)):
            filtered_idx.append(i)

    if not filtered_idx:
        # This occurs in some cases, e.g. using the Llama-3 tokenizer with a bad initialization
        raise RuntimeError(
            "No token sequences are the same after decoding and re-encoding. "
            "Consider setting `filter_ids=False` or trying a different `optim_str_init`.\n"
            "Here's an example of the token sequence that failed:\n"
            f"{ids[-1]}"
            "\n->\n"
            f"{attacks_decoded[-1]}"
            "\n->\n"
            f"{attack_ids}"
        )
    return filtered_idx