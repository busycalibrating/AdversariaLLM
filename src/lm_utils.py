import logging
import random
from functools import lru_cache
from typing import Literal

import torch
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm
from transformers import PreTrainedTokenizerBase, DynamicCache

from src.io_utils import free_vram


def generate_batched_completions(*args, **kwargs):
    raise NotImplementedError("Please migrate to generate_ragged (direct replacement), or generate_ragged_batched (with dynamic batch size adjustment).")


@torch.no_grad()
def generate_ragged_batched(
    model,
    tokenizer,
    token_list=None,
    embedding_list=None,
    initial_batch_size=64,
    use_cache=True,
    **kwargs,
) -> list[str]:
    """
    Generate completions for input_list, which can be either embeddings or tokens.
    Dynamically adjust batch size if an OOM error occurs.

    Args:
        model: The language model to use for generation.
        tokenizer: The tokenizer for the model.
        token_list: List of embeddings or tokens to generate from.
        embedding_list: List of embeddings or tokens to generate from.
        initial_batch_size: Starting batch size for generation.
        **kwargs: Additional arguments will be passed to `generate_ragged`.
    Returns:
        List of outputs generated by the model.
    """
    if token_list is not None and embedding_list is not None:
        raise ValueError("Only one of token_list or embedding_list should be provided.")
    input_type = "tokens" if token_list is not None else "embeddings"
    input_list = token_list if token_list is not None else embedding_list

    def func(chunk):
        """Wrapper function to handle a single chunk of inputs."""
        return generate_ragged(
            model=model,
            tokenizer=tokenizer,
            token_list=chunk if input_type == "tokens" else None,
            embedding_list=chunk if input_type == "embeddings" else None,
            use_cache=use_cache,
            **kwargs,
        )
    outputs = with_max_batchsize(func, input_list, initial_batch_size)
    return outputs


def with_max_batchsize(function, input, initial_batch_size=128):
    """
    Dynamically adjust batch size if an OOM error occurs.

    Args:
        function:
            A single-argument function to run.
            Takes a tensor or list of tensors and returns a tensor or list of tensors.
        input:
            The input to pass to the function, first dimension is batch dimension.
        initial_batch_size:
            Starting batch size for execution.
    Returns:
        The output of the function.
    """
    outputs = []
    i = 0
    batch_size = min(initial_batch_size, len(input))
    pbar = tqdm(total=len(input), desc=f"Running function b={batch_size}")
    while i < len(input):
        chunk = input[i : i + batch_size]
        try:
            free_vram()
            output = function(chunk)
            outputs.append(output)
            i += batch_size  # Move to the next batch
            pbar.update(batch_size)
        except torch.cuda.OutOfMemoryError:
            # If we hit OOM, reduce batch size and retry the same chunk
            batch_size = batch_size // 2
            pbar.set_description(f"Running function b={batch_size}")
            if batch_size < 1:
                raise RuntimeError(
                    "OOM even with batch_size=1; cannot generate further."
                )
    pbar.close()
    if all(isinstance(x, torch.Tensor) for x in outputs):
        outputs = torch.cat(outputs, dim=0)
    else:
        outputs = [item for sublist in outputs for item in sublist]
    assert len(outputs) == len(input)
    return outputs


@torch.no_grad
def generate_ragged(
    model,
    tokenizer,
    embedding_list=None,
    token_list=None,
    max_new_tokens: int = 256,
    return_tokens=False,
    padding_side="right",
    use_cache=True,
) -> list[str] | torch.Tensor:
    """
    Generate completions for multiple prompts in a single batch.
    No KV-cache for left-padding yet.
    Heavily tested across models to be close to individual generations.
    This is far from trivial due to various padding (left/right) and masking issues.
    The final function is still not identical to individual generations, but it is close.
    The reason for this is probably that attention masks typically don't use -inf for
    masked tokens, but instead have values like -65504 for float16.
    This can lead to small differences in the final logits and thus the generated tokens.
    We are much closer to individual generations than HF model.generate, which often
    fails in mysterious ways for LLama & Qwen models. Generations for CircuitBreakers
    often look weird, but are actually just what the model would output with `generate`
    as well.

    Number of generations that are the same as single-batch:
        Model Name                         This function    HF generate
        cais/zephyr_7b_r2d2                      100/100        100/100
        ContinuousAT/Llama-2-7B-CAT              100/100         39/100
        ContinuousAT/Phi-CAT                      90/100         95/100
        ContinuousAT/Zephyr-CAT                  100/100        100/100
        google/gemma-2-2b-it                      55/100         56/100
        meta-llama/Meta-Llama-3.1-8B-Instruct     62/100         11/100
        meta-llama/Llama-2-7b-chat-hf             88/100         30/100
        microsoft/Phi-3-mini-4k-instruct          53/100         50/100
        mistralai/Mistral-7B-Instruct-v0.3        83/100         79/100
        qwen/Qwen2-7B-Instruct                    78/100         19/100
        ---------------------------------------------------------------
        Total                                   809/1000       579/1000

    Args:
        model: A pretrained model.
        tokenizer: A pretrained tokenizer.
        embedding_list: list[torch.Tensor], optional
            A list of embeddings for each prompt. Should not be padded and can be of different lengths.
        token_list: list[torch.Tensor], optional
            A list of tokens for each prompt. Should not be padded and can be of different lengths.
        max_new_tokens: The maximum number of tokens to generate for each prompt.
    Returns:
        A list of completions for each prompt.
    """
    if embedding_list is None == token_list is None:
        raise ValueError("One of embedding_list or token_list must be provided.")
    if embedding_list is not None:
        assert all(e.ndim == 2 for e in embedding_list), "Embeddings must be 2D."
        embedding_list = [e.to(model.device) for e in embedding_list]
    if token_list is not None:
        assert all(t.ndim == 1 for t in token_list), "Tokens must be 1D."
        token_list = [t.to(model.device) for t in token_list]
        embedding_list = [
            model.get_input_embeddings()(t.unsqueeze(0))[0] for t in token_list
        ]
    assert embedding_list is not None
    # TODO: Implement KV-caching for Gemma
    if use_cache and "gemma-2" in model.name_or_path:
        logging.warning("KV-cache not implemented for Gemma 2. Disabling cache.")
        use_cache = False

    B = len(embedding_list)
    tokens = []
    generation_completed = torch.zeros(B, dtype=torch.bool)
    if padding_side == "left":
        if use_cache:
            raise NotImplementedError("KV-cache not implemented for left padding.")
        # Add left padding
        embeddings = pad_sequence(
            [e.flip(0) for e in embedding_list], batch_first=True, padding_value=0
        ).flip(1)
        padded_embeddings = F.pad(embeddings, (0, 0, 0, max_new_tokens))
        # Create attention mask and position ids
        lengths = [
            {
                "padding": embeddings.size(1) - e.size(0),
                "generation": max_new_tokens - e.size(0),
            }
            for e in embedding_list
        ]
        attention_mask = torch.stack(
            [
                torch.cat([torch.zeros(pl["padding"]), torch.ones([pl["generation"]])])
                for pl in lengths
            ]
        ).to(model.device)
        position_ids = torch.stack(
            [
                torch.cat([torch.zeros(pl["padding"]), torch.arange(pl["generation"])])
                for pl in lengths
            ]
        ).long().to(model.device)
        next_token_idx = embeddings.size(1)
        for i in range(max_new_tokens):
            outputs = model(
                inputs_embeds=padded_embeddings[:, :next_token_idx],
                attention_mask=attention_mask[:, :next_token_idx],
                position_ids=position_ids[:, :next_token_idx],
            )
            next_tokens = outputs.logits.argmax(dim=-1)[torch.arange(B), -1]
            padded_embeddings[torch.arange(B), next_token_idx] = (
                model.get_input_embeddings()(next_tokens).detach()
            )
            tokens.append(next_tokens.cpu())
            generation_completed |= next_tokens.cpu() == tokenizer.eos_token_id
            if generation_completed.all():
                logging.info(f"Early exit after {i}/{max_new_tokens} tokens.")
                break
            next_token_idx += 1
    elif padding_side == "right":
        # Add right padding
        embeddings = pad_sequence(
            [e for e in embedding_list], batch_first=True, padding_value=0
        )
        padded_embeddings = F.pad(embeddings, (0, 0, 0, max_new_tokens))
        next_token_idx = torch.tensor([e.size(0) for e in embedding_list])

        if use_cache:
            # This is the hot path so we have additional optimizations here.
            # As we generate tokens, we keep track of which prompts are completed,
            # and only generate tokens for the active prompts.
            # This is slightly (~20%) slower if all prompts have similar length,
            # but faster if prompts have different lengths and **saves VRAM**.

            # Fill prefix cache
            past_key_values = DynamicCache()
            if next_token_idx.min() > 1:
                model(
                    inputs_embeds=padded_embeddings[:, : next_token_idx.min() - 1],
                    past_key_values=past_key_values,
                    use_cache=True,
                )
            for i in range(max_new_tokens):
                # Caching with right padding is a bit tricky:
                # We have to feed more than one token at each forward pass :(.
                # Instead, we feed a 'window' from the last token of the shortest prompt
                # to the last token of the longest prompt.
                # This means that caching works best when sequences have similar length.
                active_mask = ~generation_completed
                active_mask_idx = torch.arange(B)[active_mask]
                active_embeddings = padded_embeddings[active_mask, next_token_idx.min() - 1 : next_token_idx.max()]
                logits = model(
                    inputs_embeds=active_embeddings,
                    past_key_values=past_key_values,
                    use_cache=True,
                ).logits

                next_tokens = torch.full((B,), tokenizer.eos_token_id, device=model.device)
                next_token_idx_active = next_token_idx[active_mask]
                next_tokens[active_mask] = logits[
                    torch.arange(logits.size(0)),
                    next_token_idx_active - next_token_idx.min()
                ].argmax(dim=-1)

                padded_embeddings[active_mask_idx, next_token_idx_active] = model.get_input_embeddings()(next_tokens[active_mask])
                tokens.append(next_tokens.cpu())
                continue_mask = (next_tokens.cpu() != tokenizer.eos_token_id)[active_mask]
                # have to manually crop the past_key_values to the correct length
                # since we only add a single step at a time
                for j in range(len(past_key_values.key_cache)):
                    past_key_values.key_cache[j] = past_key_values.key_cache[j][continue_mask, :, :next_token_idx.min()]
                    past_key_values.value_cache[j] = past_key_values.value_cache[j][continue_mask, :, :next_token_idx.min()]

                generation_completed |= next_tokens.cpu() == tokenizer.eos_token_id
                if generation_completed.all():
                    logging.info(f"Early exit after {i}/{max_new_tokens} tokens.")
                    break
                next_token_idx += 1
        else:
            for i in range(max_new_tokens):
                outputs = model(inputs_embeds=padded_embeddings[:, : next_token_idx.max()])
                next_tokens = outputs.logits.argmax(dim=-1)[torch.arange(B), next_token_idx - 1]
                padded_embeddings[torch.arange(B), next_token_idx] = (
                    model.get_input_embeddings()(next_tokens).detach()
                )
                tokens.append(next_tokens.cpu())
                generation_completed |= next_tokens.cpu() == tokenizer.eos_token_id
                if generation_completed.all():
                    logging.info(f"Early exit after {i}/{max_new_tokens} tokens.")
                    break
                next_token_idx += 1
    else:
        raise ValueError(f"Unknown padding_side: {padding_side}")

    tokens = torch.stack(tokens, dim=0).T
    if return_tokens:
        return tokens
    completion = tokenizer.batch_decode(tokens, skip_special_tokens=False)
    completion = [c.split(tokenizer.eos_token)[0] for c in completion]
    return completion


@torch.no_grad
def get_batched_losses(
    model,
    targets,
    embedding_list=None,
    token_list=None,
    padding_side="right",
) -> list[torch.Tensor]:
    """
    Get per-timestep losses for multiple ragged prompts in a single batch.
    No KV-cache for now.

    Args:
        model: A pretrained model.
        targets: A list of 1D tensors containing the target tokens for each prompt.
        embedding_list: list[torch.Tensor], optional
            A list of embeddings for each prompt. Should not be padded and can be of different lengths.
        token_list: list[torch.Tensor], optional
            A list of tokens for each prompt. Should not be padded and can be of different lengths.
        max_new_tokens: The maximum number of tokens to generate for each prompt.
    Returns:
        A list of completions for each prompt.
    """
    if embedding_list is None == token_list is None:
        raise ValueError("Either embedding_list or token_list must be provided.")
    if embedding_list is not None:
        assert all(e.ndim == 2 for e in embedding_list), "Embeddings must be 2D."
        embedding_list = [e.to(model.device) for e in embedding_list]
    if token_list is not None:
        assert all(t.ndim == 1 for t in token_list), "Tokens must be 1D."
        token_list = [t.to(model.device) for t in token_list]
        embedding_list = [
            model.get_input_embeddings()(t.unsqueeze(0))[0] for t in token_list
        ]
    assert embedding_list is not None
    assert all(t.ndim == 1 for t in targets), "Targets must be 1D."
    targets = [t.to(model.device) for t in targets]

    # We first pad the embeddings to the maximum context length of the model.
    B = len(embedding_list)
    if padding_side == "left":
        print("Warning: Padding side 'left' is not recommended for get_batched_losses as it may yield nans.")
        # Add left padding
        embeddings = pad_sequence(
            [e.flip(0) for e in embedding_list], batch_first=True, padding_value=0
        ).flip(1)
        targets_padded = pad_sequence(
            [t.flip(0) for t in targets], batch_first=True, padding_value=0
        ).flip(1)
        # Create attention mask and position ids
        lengths = [
            {
                "padding": embeddings.size(1) - e.size(0),
                "generation": e.size(0),
            }
            for e in embedding_list
        ]
        attention_mask = torch.stack(
            [
                torch.cat([torch.zeros(pl["padding"]), torch.ones([pl["generation"]])])
                for pl in lengths
            ]
        ).to(model.device)
        position_ids = torch.stack(
            [
                torch.cat(
                    [torch.zeros(pl["padding"]), torch.arange(pl["generation"])]
                )
                for pl in lengths
            ]
        ).long().to(model.device)

        outputs = model(
            inputs_embeds=embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
        ).logits
        losses = F.cross_entropy(
            outputs.reshape(-1, outputs.size(-1)),
            targets_padded.view(-1),
            reduction="none",
        )
        losses = losses.view(B, -1)
        losses = [losses[i, -t.size(0) : -1] for i, t in enumerate(targets)]
    elif padding_side == "right":
        # Add right padding
        embeddings = pad_sequence(
            [e for e in embedding_list], batch_first=True, padding_value=0
        )
        targets_padded = pad_sequence(
            [t for t in targets], batch_first=True, padding_value=0
        )
        outputs = model(inputs_embeds=embeddings).logits
        losses = F.cross_entropy(
            outputs.reshape(-1, outputs.size(-1)),
            targets_padded.view(-1),
            reduction="none",
        )
        losses = losses.view(B, -1)
        losses = [losses[i, : t.size(0) - 1] for i, t in enumerate(targets)]
    else:
        raise ValueError(f"Unknown padding_side: {padding_side}")

    return losses


def prepare_tokens(
    tokenizer: PreTrainedTokenizerBase,
    prompt: str,
    target: str,
    attack: str | None = None,
    placement: Literal["prompt", "suffix"] = "suffix",
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """For many attacks, we need to figure out how exactly to tokenize the input.
    Since only some models add a space or various control tokens, we have to figure
    out the exact format. We want to make sure that the first generated token is
    exactly 'Sure', and not a space or control token.

    We thus chunk the sequence into the following 5 parts (some of which may be empty):

    [PRE] + [Prompt] + [Attack] + [POST] + [Target]

    Treating prompt and attack separately is important for optimization, as we only
    want to optimize the attack part.
    Tested with:
        cais/zephyr_7b_r2d2
        google/gemma-2-2b-it
        HuggingFaceH4/zephyr-7b-beta
        meta-llama/Llama-2-7b-chat-hf
        meta-llama/Meta-Llama-3-8B-Instruct
        meta-llama/Meta-Llama-3.1-8B-Instruct
        microsoft/Phi-3-mini-4k-instruct
        mistralai/Mistral-7B-Instruct-v0.3
        qwen/Qwen2-7B-Instruct


    Parameters:
    - tokenizer: The tokenizer to use.
    - prompt: The prompt string to use.
    - target: The target string to use.
    - attack: The attack string to use.
    - placement: Where to place the attack. Can be either "prompt" or "suffix".

    Returns:
    - pre_tokens: The tokens before the prompt.
    - prompt_tokens: The tokens of the prompt.
    - attack_tokens: The tokens of the attack. <- optimize these
    - post_tokens: The tokens after the attack.
    - target_tokens: The tokens of the target string. <- apply loss here
    """
    if placement == "prompt":
        attack, prompt = prompt, ""
    elif attack is None:
        raise ValueError("If placement is 'suffix', attack must be provided.")

    # Some tokenizers and templates (e.g., allenai/Llama-3.1-Tulu-3-8B-DPO) need more
    # messages because their tokenization is more likely to have weird splits.
    for num_messages in [100, 1000, 10000]:
        pre_tokens, post_tokens, suffix_tokens = get_pre_post_suffix_tokens(tokenizer, num_messages)
        # Now we look at the actual chat by the user
        chat = [
            {"role": "user", "content": prompt + attack},
            {"role": "assistant", "content": target},
        ]
        tokenized_together = tokenize_chats([chat], tokenizer)[0]
        # We now cut the tokenized sequence into parts step-by-step.
        # First, we remove the prefix and suffix tokens, as we already know the prefix and
        # don't neeed the suffix.
        prompt_attack_post_target = tokenized_together[len(pre_tokens) : -len(suffix_tokens)]
        # We now look for the post tokens. These are between [prompt + attack] and [target].
        prompt_attack_tokens = None
        for i in range(max(len(prompt_attack_post_target) - len(post_tokens), 0)):
            if torch.all(
                prompt_attack_post_target[i : i + len(post_tokens)] == post_tokens
            ):
                prompt_attack_tokens = prompt_attack_post_target[:i]
                target_tokens = prompt_attack_post_target[i + len(post_tokens) :]
                break
        if prompt_attack_tokens is not None:
            break
    else:
        raise ValueError(
            f"Unable to find consistent tokenizer patterns for {tokenizer.name_or_path}"
        )

    chat_no_attack = [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": target},
    ]
    tokenized_together_no_attack = tokenize_chats([chat_no_attack], tokenizer)[0]

    attack_length = len(tokenized_together) - len(tokenized_together_no_attack)

    prompt_tokens, attack_tokens = torch.tensor_split(
        prompt_attack_tokens, [prompt_attack_tokens.size(0)-attack_length]
    )
    if "llama-2" in tokenizer.name_or_path.lower():
        # LLama 2 models have incorrect templating and need to be fixed manually
        post_tokens = torch.cat([post_tokens, torch.tensor([29871])])

    return pre_tokens, prompt_tokens, attack_tokens, post_tokens, target_tokens


def _make_random_chats(n, k=5):
    generate_random_string = lambda: "".join(
        random.choices(
            " ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789", k=k
        )
    )
    return [
        [
            {"role": "user", "content": generate_random_string()},
            {"role": "assistant", "content": generate_random_string()},
        ]
        for _ in range(n)
    ]

def _extract_prefix_middle_suffix(vectors):
    def longest_common_prefix(sequences):
        if not sequences:
            return []
        prefix = sequences[0]
        for seq in sequences[1:]:
            min_len = min(len(prefix), len(seq))
            i = 0
            while i < min_len and prefix[i] == seq[i]:
                i += 1
            prefix = prefix[:i]
            if not prefix:
                return []
        return prefix

    def longest_common_suffix(sequences):
        if not sequences:
            return []
        suffix = sequences[0]
        for seq in sequences[1:]:
            min_len = min(len(suffix), len(seq))
            i = 1
            while i <= min_len and suffix[-i] == seq[-i]:
                i += 1
            if i > 1:
                suffix = suffix[-(i - 1) :]
            else:
                return []
        return suffix

    def longest_common_subsequence(sequences):
        if not sequences:
            return []
        reference = sequences[0]
        n = len(reference)
        # Start with the longest possible substrings and decrease length
        for length in range(n, 0, -1):
            for start in range(n - length + 1):
                candidate = reference[start : start + length]
                if all(
                    any(
                        candidate == seq[i : i + length]
                        for i in range(len(seq) - length + 1)
                    )
                    for seq in sequences[1:]
                ):
                    return candidate
        return []

    sequences = [vec.tolist() for vec in vectors]
    prefix = longest_common_prefix(sequences)
    suffix = longest_common_suffix(sequences)
    # Trim the prefix and suffix from sequences
    sequences_trimmed = [
        seq[len(prefix) : len(seq) - len(suffix) if len(suffix) > 0 else None]
        for seq in sequences
    ]
    middle = longest_common_subsequence(sequences_trimmed)
    return torch.tensor(prefix), torch.tensor(middle), torch.tensor(suffix)


def tokenize_chats(chats: list[list[dict[str,str]]], tokenizer) -> list[torch.Tensor]:
    templates = tokenizer.apply_chat_template(
        chats, tokenize=False, add_generation_prompt=False
    )
    # Sometimes, the chat template adds the BOS token to the beginning of the template.
    # The tokenizer adds it again later, so we need to remove it to avoid duplication.
    if tokenizer.bos_token:
        for i, template in enumerate(templates):
            templates[i] = template.removeprefix(tokenizer.bos_token)

    return [
        tokenizer(t, return_tensors="pt", add_special_tokens=True).input_ids[0]
        for t in templates
    ]


# Generate random messages to find tokenizer patterns, this is ugly but fast
@lru_cache()
def get_pre_post_suffix_tokens(tokenizer, num_messages):
    test_chats = _make_random_chats(num_messages)
    test_tokenized = tokenize_chats(test_chats, tokenizer)
    return _extract_prefix_middle_suffix(test_tokenized)
