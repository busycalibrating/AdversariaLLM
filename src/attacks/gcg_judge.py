"""Single-file implementation of GCG with a judge as objective.
"""
import gc
import logging
import time
from dataclasses import dataclass
from typing import Literal

import torch
import transformers
from accelerate.utils import find_executable_batch_size
from torch import Tensor
from tqdm import trange

from src.lm_utils import generate_ragged_batched, prepare_tokens
from src.judges import judge_strong_reject
from transformers import AutoModelForCausalLM, AutoTokenizer

from .attack import Attack, AttackResult


@dataclass
class GCGJudgeConfig:
    name: str = "gcg_judge"
    type: str = "discrete"
    version: str = ""
    placement: str = "suffix"
    generate_completions: Literal["all", "best", "last"] = "all"
    num_steps: int = 250
    seed: int = 0
    batch_size: int = 512
    optim_str_init: str = "x x x x x x x x x x x x x x x x x x x x"
    search_width: int = 512
    topk: int = 256
    n_replace: int = 1
    buffer_size: int = 0
    use_constrained_gradient: bool = False
    early_stop: bool = False
    use_prefix_cache: bool = True
    allow_non_ascii: bool = False
    allow_special: bool = False
    filter_ids: bool = True
    verbosity: str = "WARNING"
    token_selection: str = "default"
    max_new_tokens: int = 256


def get_disallowed_ids(tokenizer, allow_non_ascii, allow_special, device="cpu"):
    disallowed_ids = []

    def is_ascii(s):
        return s.isascii() and s.isprintable()

    # Important to loop over len(tokenizer), not just tokenizer.vocab_size, because
    # special tokens added post-hoc are not counted to vocab_size.
    if not allow_non_ascii:
        for i in range(len(tokenizer)):
            if not is_ascii(tokenizer.decode([i])):
                disallowed_ids.append(i)

    if not allow_special:
        for i in range(len(tokenizer)):
            if not tokenizer.decode([i], skip_special_tokens=True):
                disallowed_ids.append(i)

    if tokenizer.bos_token_id is not None:
        disallowed_ids.append(tokenizer.bos_token_id)
    if tokenizer.eos_token_id is not None:
        disallowed_ids.append(tokenizer.eos_token_id)
    if tokenizer.pad_token_id is not None:
        disallowed_ids.append(tokenizer.pad_token_id)
    if tokenizer.unk_token_id is not None:
        disallowed_ids.append(tokenizer.unk_token_id)
    disallowed_ids = sorted(list(set(disallowed_ids)))
    return torch.tensor(disallowed_ids, device=device)


def filter_ids(ids: Tensor, tokenizer: transformers.PreTrainedTokenizer, prompt, target):
    """Filters out sequences of token ids that change after retokenization.

    Args:
        ids : Tensor, shape = (search_width, n_optim_ids)
        tokenizer : ~transformers.PreTrainedTokenizer
        prompt : str
        target : str

    Returns:
        filtered_ids : Tensor, shape = (new_search_width, n_optim_ids)
            all token ids that are the same after retokenization
    """
    attacks_decoded = tokenizer.batch_decode(ids)
    filtered_idx = []

    for i, attack in enumerate(attacks_decoded):
        pre_ids, prompt_ids, attack_ids, post_ids, target_ids = prepare_tokens(
            tokenizer,
            prompt,
            target,
            attack=attack,
            placement="suffix",
        )
        if torch.equal(ids[i], attack_ids.to(ids.device)):
            filtered_idx.append(i)

    if not filtered_idx:
        # This occurs in some cases, e.g. using the Llama-3 tokenizer with a bad initialization
        raise RuntimeError(
            "No token sequences are the same after decoding and re-encoding. "
            "Consider setting `filter_ids=False` or trying a different `optim_str_init`"
            "An example of the token sequence that failed:"
            f"{ids[-1]}"
            "\n->\n"
            f"{attacks_decoded[-1]}"
            "\n->\n"
            f"{attack_ids}"
        )
    return filtered_idx


class GCGJudgeAttack(Attack):
    def __init__(self, config: GCGJudgeConfig):
        super().__init__(config)

        self.classifier = AutoModelForCausalLM.from_pretrained(
            "qylu4156/strongreject-15k-v1",
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "qylu4156/strongreject-15k-v1",
            use_fast=False,
            truncation_side="right",
            padding_side="left"
        )
        self.logger = logging.getLogger("nanogcg")
        if not self.logger.hasHandlers():
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s [%(filename)s:%(lineno)d] %(message)s",
                datefmt="%Y-%m-%d %H:%M:%S",
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

    def run(self, model, tokenizer, dataset) -> AttackResult:
        not_allowed_ids = get_disallowed_ids(tokenizer, self.config.allow_non_ascii, self.config.allow_special, device=model.device)
        results = AttackResult([], [], [], [], [])
        for msg, target in dataset:
            msg: dict[str, str]
            target: str
            t0 = time.time()

            pre_ids, prompt_ids, attack_ids, post_ids, _ = prepare_tokens(
                tokenizer,
                msg["content"],
                "",
                attack=self.config.optim_str_init,
                placement="suffix",
            )
            self.pre_ids = pre_ids.unsqueeze(0).to(model.device)
            self.prompt_ids = prompt_ids.unsqueeze(0).to(model.device)
            self.pre_prompt_ids = torch.cat([self.pre_ids, self.prompt_ids], dim=1)
            attack_ids = attack_ids.unsqueeze(0).to(model.device)
            self.post_ids = post_ids.unsqueeze(0).to(model.device)

            # Embed everything that doesn't get optimized
            embedding_layer = model.get_input_embeddings()
            pre_prompt_embeds, post_embeds = [
                embedding_layer(ids) for ids in (self.pre_prompt_ids, self.post_ids)
            ]

            # Compute the KV Cache for tokens that appear before the optimized tokens
            if self.config.use_prefix_cache and model.name_or_path != "google/gemma-2-2b-it":
                with torch.no_grad():
                    output = model(inputs_embeds=pre_prompt_embeds, use_cache=True)
                    self.prefix_cache = output.past_key_values
            else:
                self.prefix_cache = None

            self.pre_prompt_embeds = pre_prompt_embeds
            self.post_embeds = post_embeds

            # Initialize the attack buffer
            buffer = self.init_buffer(model, attack_ids)
            optim_ids = buffer.get_best_ids()
            token_selection = SubstitutionSelectionStrategy(self.config.token_selection, self.config, self.prefix_cache, self.pre_prompt_embeds, self.post_embeds)

            losses = []
            times = []
            optim_strings = []
            self.stop_flag = False

            for _ in (pbar := trange(self.config.num_steps)):
                # Compute the token gradient
                sampled_ids, sampled_ids_pos = token_selection(
                    optim_ids.squeeze(0),
                    model,
                    self.config.search_width,
                    self.config.topk,
                    self.config.n_replace,
                    not_allowed_ids=not_allowed_ids,
                )
                with torch.no_grad():
                    # Sample candidate token sequences
                    if self.config.filter_ids:
                        # We're trying to be as strict as possible here, so we filter
                        # the entire prompt, not just the attack sequence in an isolated
                        # way. This is because the prompt and attack can affect each
                        # other's tokenization in some cases.
                        idx = filter_ids(
                            sampled_ids,
                            tokenizer,
                            msg["content"],
                            target,
                        )
                        sampled_ids = sampled_ids[idx]
                        sampled_ids_pos = sampled_ids_pos[idx]

                    new_search_width = sampled_ids.shape[0]

                    # Compute loss on all candidate sequences
                    batch_size = (
                        new_search_width
                        if self.config.batch_size is None
                        else self.config.batch_size
                    )

                    loss = find_executable_batch_size(
                        self.compute_candidates_score, batch_size
                    )(sampled_ids, model, tokenizer)
                    # Create and save violin plot

                    current_loss = loss.max().item()
                    optim_ids = sampled_ids[loss.argmax()].unsqueeze(0)

                    # Update the buffer based on the loss
                    # print(current_loss, loss)
                    losses.append(current_loss)
                    times.append(time.time() - t0)
                    if buffer.size == 0 or current_loss > buffer.get_lowest_loss():
                        buffer.add(current_loss, optim_ids)

                optim_ids = buffer.get_best_ids()
                optim_str = tokenizer.batch_decode(optim_ids)[0]
                optim_strings.append(optim_str)
                pbar.set_postfix({"Loss": current_loss, "Best Attack": optim_str[:50]})

                # Store loss distribution for violin plot
                # violin_data.append(loss.cpu().numpy())
                # fig, ax = plt.subplots(figsize=(10, 6))
                # ax.violinplot(violin_data, positions=range(1, len(violin_data) + 1), showmeans=True)
                # ax.set_xlabel('Optimization Step')
                # ax.set_ylabel('Loss Value')
                # ax.set_title('Distribution of Loss Values Over Time')
                # plt.savefig('loss_distribution.pdf', bbox_inches='tight')
                # plt.close()

                if self.stop_flag:
                    self.logger.info("Early stopping due to finding a perfect match.")
                    break

            # Generate completions
            match self.config.generate_completions:
                case "all":
                    attacks = optim_strings
                case "best":
                    attacks = [optim_strings[losses.index(min(losses))]]
                case "last":
                    attacks = [optim_strings[-1]]
                case _:
                    raise ValueError(
                        f"Unknown value for generate_completions: {self.config.generate_completions}"
                    )

            token_list = [
                torch.cat(prepare_tokens(
                    tokenizer,
                    prompt=msg["content"],
                    target="",
                    attack=attack,
                )[:4])
                for attack in attacks
            ]
            completions = generate_ragged_batched(
                model,
                tokenizer,
                token_list=token_list,
                initial_batch_size=self.config.batch_size,
                max_new_tokens=self.config.max_new_tokens
            )
            results.losses.append(losses)
            results.attacks.append(optim_strings)
            results.prompts.append(msg)
            results.completions.append(completions)
            results.times.append(times)
        return results

    def init_buffer(self, model, init_buffer_ids):
        config = self.config

        # Create the attack buffer and initialize the buffer ids
        buffer = AttackBuffer(config.buffer_size)
        true_buffer_size = max(1, config.buffer_size)

        init_buffer_scores = find_executable_batch_size(
            self.compute_candidates_score, true_buffer_size
        )(init_buffer_ids, model, self.tokenizer)

        # Populate the buffer
        for i in range(true_buffer_size):
            buffer.add(init_buffer_scores[i], init_buffer_ids[[i]])
        return buffer

    def compute_candidates_score(
        self,
        search_batch_size: int,
        sampled_ids: Tensor,
        model: transformers.PreTrainedModel,
        tokenizer: transformers.PreTrainedTokenizer,
    ) -> Tensor:
        """Computes the GCG loss on all candidate token id sequences.

        Args:
            search_batch_size : int
                the number of candidate sequences to evaluate in a given batch
            input_embeds : Tensor, shape = (search_width, seq_len, embd_dim)
                the embeddings of the `search_width` candidate sequences to evaluate
        """
        all_score = []

        for i in range(0, sampled_ids.shape[0], search_batch_size):
            with torch.no_grad():
                sampled_ids_batch = sampled_ids[i : i + search_batch_size]
                current_batch_size = sampled_ids_batch.shape[0]

                completions = generate_ragged_batched(
                    model,
                    tokenizer,
                    token_list=[torch.cat([self.pre_ids, self.prompt_ids, s_ids.unsqueeze(0), self.post_ids], dim=1)[0] for s_ids in sampled_ids_batch],
                    max_new_tokens=64,
                    temperature=0.0,
                )
                prompts = [{"content": c} for c in tokenizer.batch_decode(torch.cat([self.prompt_ids.expand(current_batch_size, -1), sampled_ids_batch], dim=1))]
                score = judge_strong_reject(current_batch_size, self.classifier, self.tokenizer, prompts, completions)[0]
                for c, l in zip(completions, score):
                    print(l, c)
                all_score.append(torch.tensor(score, device=model.device))

                gc.collect()
                torch.cuda.empty_cache()
        return torch.cat(all_score, dim=0)


class AttackBuffer:
    def __init__(self, size: int):
        self.buffer = []  # elements are (loss: float, optim_ids: Tensor)
        self.size = size

    def add(self, loss: float, optim_ids: Tensor) -> None:
        if self.size == 0:
            self.buffer = [(loss, optim_ids)]
            return

        if len(self.buffer) < self.size:
            self.buffer.append((loss, optim_ids))
        else:
            self.buffer[-1] = (loss, optim_ids)

        self.buffer.sort(key=lambda x: x[0], reverse=True)

    def get_best_ids(self) -> Tensor:
        return self.buffer[0][1]

    def get_lowest_loss(self) -> float:
        return self.buffer[0][0]

    def get_highest_loss(self) -> float:
        return self.buffer[-1][0]


class SubstitutionSelectionStrategy:
    def __init__(self, strategy: str, config: GCGJudgeConfig, prefix_cache: list[tuple[Tensor, Tensor]], pre_prompt_embeds: Tensor, post_embeds: Tensor, target_embeds: Tensor | None = None, target_ids: Tensor | None = None):
        self.config = config
        self.strategy = strategy
        self.prefix_cache = prefix_cache
        self.pre_prompt_embeds = pre_prompt_embeds
        self.post_embeds = post_embeds
        self.target_embeds = target_embeds
        self.target_ids = target_ids

    def __call__(
        self,
        ids: Tensor,
        model: transformers.PreTrainedModel,
        search_width: int,
        topk: int,
        n_replace: int,
        not_allowed_ids: Tensor,
        *args,
        **kwargs,
    ):
        if self.strategy == "default":
            return self._sample_ids_from_grad(
                ids,
                model,
                search_width,
                topk,
                n_replace,
                not_allowed_ids,
                *args,
                **kwargs,
            )
        elif self.strategy == "random_overall":
            return self._random_overall(
                ids,
                model,
                search_width,
                topk,
                n_replace,
                not_allowed_ids,
                *args,
                **kwargs,
            )
        elif self.strategy == "random_per_position":
            return self._random_per_position(
                ids,
                model,
                search_width,
                topk,
                n_replace,
                not_allowed_ids,
                *args,
                **kwargs,
            )
        else:
            raise ValueError(f"Invalid replacement selection strategy: {self.strategy}")

    def _sample_ids_from_grad(
        self,
        ids: Tensor,
        model: transformers.PreTrainedModel,
        search_width: int,
        topk: int = 256,
        n_replace: int = 1,
        not_allowed_ids: Tensor | None = None,
    ):
        """Returns `search_width` combinations of token ids based on the token gradient.
        Original GCG does this.

        Args:
            ids : Tensor, shape = (n_optim_ids)
                the sequence of token ids that are being optimized
            grad : Tensor, shape = (n_optim_ids, vocab_size)
                the gradient of the GCG loss computed with respect to the one-hot token embeddings
            search_width : int
                the number of candidate sequences to return
            topk : int
                the topk to be used when sampling from the gradient
            n_replace: int
                the number of token positions to update per sequence
            not_allowed_ids: Tensor, shape = (n_ids)
                the token ids that should not be used in optimization

        Returns:
            sampled_ids : Tensor, shape = (search_width, n_optim_ids)
                sampled token ids
        """
        grad = self.compute_token_gradient(ids.unsqueeze(0), model).squeeze(0)  # (n_optim_ids, vocab_size)
        n_optim_tokens = len(ids)
        original_ids = ids.repeat(search_width, 1)

        if not_allowed_ids is not None:
            grad[:, not_allowed_ids.to(grad.device)] = float("inf")
        # (n_optim_ids, topk)
        topk_ids = grad.topk(topk, dim=1, largest=False, sorted=False).indices

        sampled_ids_pos = torch.randint(
            0, n_optim_tokens, (search_width, n_replace), device=grad.device
        )  # (search_width, n_replace)
        sampled_topk_idx = torch.randint(
            0, topk, (search_width, n_replace, 1), device=grad.device
        )

        sampled_ids_val = (
            topk_ids[sampled_ids_pos].gather(2, sampled_topk_idx).squeeze(2)
        )  # (search_width, n_replace)

        new_ids = original_ids.scatter_(
            1, sampled_ids_pos, sampled_ids_val
        )  # (search_width, n_optim_ids)

        return new_ids, sampled_ids_pos

    @torch.no_grad()
    def _random_overall(
        self,
        ids: Tensor,
        model: transformers.PreTrainedModel,
        search_width: int,
        topk: int = 256,
        n_replace: int = 1,
        not_allowed_ids: Tensor | None = None,
    ):
        """Returns `search_width` random token substitutions.

        Args:
            ids : Tensor, shape = (n_optim_ids,)
                the sequence of token ids that are being optimized
            grad : Tensor, shape = (n_optim_ids, vocab_size)
                the gradient of the GCG loss computed with respect to the one-hot token embeddings
            search_width : int
                the number of candidate sequences to return
            topk : int
                the topk to be used when sampling from the gradient
            n_replace: int
                the number of token positions to update per sequence
            not_allowed_ids: Tensor, shape = (n_ids,)
                the token ids that should not be used in optimization

        Returns:
            sampled_ids : Tensor, shape = (search_width, n_optim_ids)
                sampled token ids
        """
        vocab_size = model.get_input_embeddings().weight.size(0)
        n_optim_tokens = ids.shape[0]
        original_ids = ids.repeat(search_width, 1)

        # Create valid token mask
        valid_tokens = torch.ones(vocab_size, dtype=torch.bool, device=ids.device)
        if not_allowed_ids is not None:
            valid_tokens[not_allowed_ids.to(ids.device)] = False

        # Sample positions and token indices
        sampled_ids_pos = torch.randint(0, n_optim_tokens, (search_width, 1), device=ids.device)
        valid_token_indices = torch.nonzero(valid_tokens).squeeze()
        sampled_topk_idx = valid_token_indices[torch.randint(0, valid_token_indices.size(0), (search_width, 1), device=ids.device)]

        # Create new sequences with substitutions
        new_ids = original_ids.scatter_(1, sampled_ids_pos, sampled_topk_idx)

        return new_ids, sampled_ids_pos

    @torch.no_grad()
    def _random_per_position(
        self,
        ids: Tensor,
        model: transformers.PreTrainedModel,
        search_width: int,
        topk: int = 256,
        n_replace: int = 1,
        not_allowed_ids: Tensor | None = None,
    ):
        """Returns `search_width` random token substitutions.

        Args:
            ids : Tensor, shape = (n_optim_ids,)
                the sequence of token ids that are being optimized
            model : transformers.PreTrainedModel
                the model to compute the gradient with respect to
            search_width : int
                the number of candidate sequences to return
            topk : int
                the topk to be used when sampling from the gradient
            n_replace: int
                the number of token positions to update per sequence
            not_allowed_ids: Tensor, shape = (n_ids,)
                the token ids that should not be used in optimization

        Returns:
            sampled_ids : Tensor, shape = (search_width, n_optim_ids)
                sampled token ids
        """
        # Sample search_width//ids.shape[0] substitutions at each position
        samples_per_position = search_width // ids.shape[0]
        positions = torch.arange(ids.shape[0], device=ids.device)
        original_ids = ids.repeat(search_width, 1)

        # Get valid ids for each position (all except not_allowed_ids)
        valid_ids = torch.ones((ids.shape[0], model.get_input_embeddings().weight.size(0)), dtype=torch.bool, device=ids.device)
        if not_allowed_ids is not None:
            valid_ids[:, not_allowed_ids.to(ids.device)] = False

        # Sample indices for each position in parallel
        sampled_ids = torch.empty((ids.shape[0], samples_per_position), dtype=torch.long, device=ids.device)
        rand_perm = torch.argsort(torch.rand_like(valid_ids.float()), dim=1)
        valid_perm = torch.masked_select(rand_perm, valid_ids).reshape(ids.shape[0], -1)
        sampled_ids = valid_perm[:, :samples_per_position]

        # Reshape to (total_samples, 1) format
        sampled_topk_idx = sampled_ids.reshape(-1)
        sampled_ids_pos = positions.repeat_interleave(samples_per_position)
        original_ids = original_ids[:samples_per_position * ids.shape[0]]
        new_ids = original_ids.scatter_(1, sampled_ids_pos.unsqueeze(1), sampled_topk_idx.unsqueeze(1))
        return new_ids, sampled_ids_pos

    def compute_token_gradient(
        self,
        optim_ids: Tensor,
        model: transformers.PreTrainedModel,
    ) -> Tensor:
        """Computes the gradient of the GCG loss w.r.t the one-hot token matrix.

        Args:
        optim_ids : Tensor, shape = (N, n_optim_ids)
            the sequence of token ids that are being optimized
        model : transformers.PreTrainedModel
            the model to compute the gradient with respect to

        Returns:
            grad : Tensor, shape = (N, n_optim_ids, vocab_size)
                the gradient of the GCG loss computed with respect to the one-hot token embeddings
        """
        embedding_layer = model.get_input_embeddings()

        # Create the one-hot encoding matrix of our optimized token ids
        optim_ids_onehot = torch.nn.functional.one_hot(
            optim_ids, num_classes=embedding_layer.num_embeddings
        )
        optim_ids_onehot = optim_ids_onehot.to(dtype=model.dtype, device=model.device)
        optim_ids_onehot.requires_grad_()

        # (1, num_optim_tokens, vocab_size) @ (vocab_size, embed_dim) -> (1, num_optim_tokens, embed_dim)
        if self.config.use_constrained_gradient:
            optim_embeds = (
                optim_ids_onehot / optim_ids_onehot.sum(dim=-1, keepdim=True)
            ) @ embedding_layer.weight
        else:
            optim_embeds = optim_ids_onehot @ embedding_layer.weight

        if self.prefix_cache:
            input_embeds = torch.cat(
                [optim_embeds, self.post_embeds, self.target_embeds], dim=1
            )
            output = model(
                inputs_embeds=input_embeds, past_key_values=self.prefix_cache, use_cache=True
            )
        else:
            input_embeds = torch.cat(
                [
                    self.pre_prompt_embeds,
                    optim_embeds,
                    self.post_embeds,
                    self.target_embeds,
                ],
                dim=1,
            )
            output = model(inputs_embeds=input_embeds)
        logits = output.logits

        # Shift logits so token n-1 predicts token n
        shift = input_embeds.shape[1] - self.target_ids.shape[1]
        shift_logits = logits[
            ..., shift - 1 : -1, :
        ].contiguous()  # (1, num_target_ids, vocab_size)
        shift_labels = self.target_ids

        loss = torch.nn.functional.cross_entropy(
            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)
        )

        optim_ids_onehot_grad = torch.autograd.grad(
            outputs=[loss],
            inputs=[optim_ids_onehot],
            create_graph=False,
            retain_graph=False
        )[0]
        return optim_ids_onehot_grad
